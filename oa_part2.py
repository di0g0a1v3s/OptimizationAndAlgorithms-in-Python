# -*- coding: utf-8 -*-
"""OA_Part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10_oHTYs5RN_wROjDnFWohPmWapJ-nfOs

# **Part 2** - Optimization and Algorithms Project
"""

# Import packages.
import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sp
import numpy as np

"""## 2 - Gradient Descent Method"""

def gradient_descent(f,grad_f,s_0,x,y,max_it,err_max,alpha_in,beta,lam):
    ## Initializations
    s = s_0.copy()
    n = s.shape[1] # find dimension of the problem
    grad_norm = []
    alpha = alpha_in

    ## Start Gradient Descent iterations
    for l in range(max_it):

        # Calculate Gradient
        grad = grad_f(s,x,y)

        # Save the norm of the gradient in list
        grad_norm.append(np.linalg.norm(grad.reshape(n), n))

        # Check stopping criterion
        if grad_norm[l] < err_max:
            return s, n, grad_norm
        
        d = -grad
        
        # Backtracking routine
        alpha = alpha_in # Reset alpha value
        while f(s+alpha*d,x,y) >= f(s,x,y) + lam*(grad@(alpha*d.T)):
            alpha = alpha*beta

        # Update
        s += alpha * d

    return np.zeros((1,n)), n, 0

"""### Fucntions to plot results"""

def plot_GDM(x,y,s,grad,mode):
    r = s[0,s.shape[1]-1]
    s = np.delete(s, s.shape[1]-1)
    print("s = ",s,", r = ",r)

    # Only plot points and line when mode is 0 (2 dimension problem)
    if mode == 0:
        # Plot data and fitted line
        plt.figure(1)
        for n in range(k):
            if y[0,n] == 0:
                plt.plot(x[0,n], x[1,n], 'ro')
            else:
                plt.plot(x[0,n], x[1,n], 'bo')

        # Add fitted line
        xa = np.linspace(np.amin(x[0,:]), np.amax(x[0,:]), 30)
        ya = (-s[0]*xa + r)/s[1]
        plt.plot(xa, ya, 'g--')

        plt.show()
    
    # Plot norm of gradient over the iterations
    plt.figure(2)
    plt.yscale('log')
    plt.plot(range(len(grad)),grad)
    plt.xlabel('Iteration')
    plt.ylabel('Gradient Norm')
    plt.title('Gradient Norm along the iterations')
    plt.grid(True, which="both", ls="-")
    plt.show()

"""Define cost function and its' derivative"""

# Define cost function
f = lambda s,x,y: np.sum( np.log(1+np.exp(s@x)) - y*(s@x) )/k

# Define derivative of cost function
def grad_f(s,x,y): 
    grad = ( np.exp(s@x)/(1+np.exp(s@x)) @ x.T - y@x.T )/k
    return grad

"""### Task 2

Start by importing the data needed
"""

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data1.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)

"""Define algorithm parameters and run the optimization"""

# Define Parameters
max_it = 100000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)


# Define initial conditions
r_0 = 0.0
s_0 = np.array([[-1.0,-1.0,r_0]])

# Solve problem using gradient descent method
[s,n,grad_norm] = gradient_descent(f,grad_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_GDM(x,y,s,grad_norm,0)

"""###Task 3"""

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data2.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)


# Define Parameters
max_it = 100000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)

# Define initial conditions
r_0 = 0.0
s_0 = np.array([[-1.0,-1.0,r_0]])

# Solve problem using gradient descent method
[s,n,grad_norm] = gradient_descent(f,grad_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_GDM(x,y,s,grad_norm,0)

"""### Task 4"""

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data3.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]
n = x.shape[0]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)

# Define Parameters
max_it = 100000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)

# Define initial conditions
r_0 = np.array([[0.0]])
s_0 = np.concatenate((-1*np.ones((1,n)),r_0),axis=1)

# Solve problem using gradient descent method
[s,n,grad_norm] = gradient_descent(f,grad_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_GDM(x,y,s,grad_norm,1)

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data4.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]
n = x.shape[0]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)

# Define Parameters
max_it = 100000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)

# Define initial conditions
r_0 = np.array([[0.0]])
s_0 = np.concatenate((-1*np.ones((1,n)),r_0),axis=1)

# Solve problem using gradient descent method
[s,n,grad_norm] = gradient_descent(f,grad_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_GDM(x,y,s,grad_norm,1)

"""## 3 - Newton Method"""

def newton_method(f,grad_f,grad2_f,s_0,x,y,max_it,err_max,alpha_in,beta,lam):
    ## Initializations
    s = s_0.copy()
    n = s.shape[1]
    grad_norm = []
    alpha = alpha_in
    alpha_out = []

    ## Start Gradient Descent iterations
    for l in range(max_it):

        # Calculate Gradient and Hessian matrix
        grad = grad_f(s,x,y)
        hessian = hessian_f(s,x,y)
        
        # Save the norm of the gradient and learning rate in lists
        grad_norm.append(np.linalg.norm(grad.reshape(n), n))
        alpha_out.append(alpha)

        # Check stopping criterion
        if grad_norm[l] < err_max:
            return s, l, grad_norm, alpha_out

        d = np.linalg.solve(hessian,-1*grad.T)
        
        # Backtracking routine
        alpha = alpha_in # Reset alpha value
        while f(s+alpha*d.T,x,y) >= f(s,x,y) + lam*(grad@(alpha*d)):
            alpha = alpha*beta

        # Update
        s += alpha * d.T

    return np.zeros((1,n)), l, grad_norm, alpha_out

def plot_NM(s,grad,alpha):
    print("s = ",s,", r = ",s[0,s.shape[1]-1])

    # Plot norm of gradient over the iterations
    plt.figure(1)
    plt.yscale('log')
    plt.plot(range(len(grad)),grad)
    plt.xlabel('Iteration')
    plt.ylabel('Gradient Norm')
    plt.title('Gradient Norm along the iterations')
    plt.grid(True, which="both", ls="-")
    plt.show()

    plt.figure(2)
    plt.plot(range(len(alpha)), alpha,'-o')
    plt.xlabel('Iteration')
    plt.ylabel(r'$\alpha_k$')
    plt.title(r'$\alpha_k$ along the iterations (Newton Method)')
    plt.show()

# Hessian Matrix Calculation
def hessian_f(s,x,y): 
    k = y.shape[1]

    # Calculate vector of second derivatives (for each of the data point)
    deriv = 1/k*np.exp(s@x) / ((1+np.exp(s@x))*(1+np.exp(s@x))) 

    # Create diagonal matrix of second derivatives
    D = np.diag(deriv[0])
    
    # Calculate hessian matrix
    hessian = x@D@x.T
    return hessian

"""### Task 6

#### data1.mat
"""

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data1.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]
n = x.shape[0]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)

# Define Parameters
max_it = 1000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)

# Define initial conditions
r_0 = np.array([[0.0]])
s_0 = np.concatenate((-1*np.ones((1,n)),r_0),axis=1)

# Solve problem using gradient descent method
[s,n,grad_norm,alpha] = newton_method(f,grad_f,hessian_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_NM(s,grad_norm,alpha)

"""#### data2.mat"""

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data2.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]
n = x.shape[0]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)

# Define Parameters
max_it = 1000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)

# Define initial conditions
r_0 = np.array([[0.0]])
s_0 = np.concatenate((-1*np.ones((1,n)),r_0),axis=1)

# Solve problem using gradient descent method
[s,n,grad_norm,alpha] = newton_method(f,grad_f,hessian_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_NM(s,grad_norm,alpha)

"""#### data3.mat"""

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data3.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]
n = x.shape[0]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)

# Define Parameters
max_it = 1000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)

# Define initial conditions
r_0 = np.array([[0.0]])
s_0 = np.concatenate((-1*np.ones((1,n)),r_0),axis=1)

# Solve problem using gradient descent method
[s,n,grad_norm,alpha] = newton_method(f,grad_f,hessian_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_NM(s,grad_norm,alpha)

"""#### data4.mat"""

# Import data (for matlab .mat file)
mat_contents = sp.loadmat("data4.mat")

x = mat_contents['X']
y = mat_contents['Y']
# Number of data points
k = y.shape[1]
n = x.shape[0]

# Add line with all values -1 (to multiply by the r contained in the s vector)
x = np.concatenate((x, -1*np.ones((1,k))), axis=0)

# Define Parameters
max_it = 1000
er_max = 10**(-6)
alpha = 1.0
beta = 0.5
lam = 10**(-4)

# Define initial conditions
r_0 = np.array([[0.0]])
s_0 = np.concatenate((-1*np.ones((1,n)),r_0),axis=1)

# Solve problem using gradient descent method
[s,n,grad_norm,alpha] = newton_method(f,grad_f,hessian_f,s_0,x,y,max_it,er_max,alpha,beta,lam)

# Plot Results
plot_NM(s,grad_norm,alpha)